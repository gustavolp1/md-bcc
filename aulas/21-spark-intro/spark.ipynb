{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o ambiente\n",
    "\n",
    "### Opção 1 - Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O jeito mais simples de começar a trabalhar com Spark é instalar um container com tudo pronto! No site https://hub.docker.com/r/jupyter/pyspark-notebook vemos uma imagem Docker que já vem com `pyspark` e `jupyter lab`. Instale a imagem com o comando:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos iniciar o ambiente de trabalho com o comando `docker run`. Para isso precisamos tomar alguns cuidados:\n",
    "\n",
    "1) Temos que mapear nosso diretorio local de trabalho para um diretório interno do container, de modo que alterações feitas dentro do container (nesta pasta escolhida) sejam gravadas no nosso diretorio local. No container temos um usuário padrão com *username* `jovyan`. No *homedir* desse usuario temos uma pasta vazia `work`, que vai servir como local de mapeamento do nosso diretorio local de trabalho. Podemos então fazer esse mapeamendo com a opção `-v` do comando `docker run` da seguinte forma:\n",
    "\n",
    "```bash\n",
    "-v <diretorio>:/home/jovyan/work\n",
    "```\n",
    "\n",
    "onde `<diretorio>` representa seu diretorio local de trabalho.\n",
    "\n",
    "2) Para acessar o `jupyter notebook` e o *dashboard* do Spark a partir do nosso *browser* favorito temos que abrir algumas portas do container com a opção `-p`. As portas são `8888` (para o próprio `jupyter notebook`) e `4040` (para o *dashboard* do Spark). Ou seja, adicionaremos às opções do `docker run`o seguinte:\n",
    "\n",
    "```bash\n",
    "-p 8888:8888 -p 4040:4040\n",
    "```\n",
    "\n",
    "Desta forma, ao acessar `localhost:8888` na nossa máquina, estaremos acessando o servidor Jupyter na porta 8888 interna do container.\n",
    "\n",
    "3) Vamos iniciar o container no modo interativo, e vamos especificar que o container deve ser encerrado ao fechar o servidor Jupyter. Faremos isso com as opções `-it` e `-rm`\n",
    "\n",
    "Antes de executar, garanta que as portas 4040 e 8888 estão livres (sem jupyter já executando) ou altere o comando. Ainda, esteja na pasta da aula ao executar, assim apenas ela será exposta ao container.\n",
    "\n",
    "Portanto, o comando completo que eu uso na minha máquina Linux para iniciar o container é:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v \"`pwd`\":/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estiver no Windows estes comandos, utilize:\n",
    "\n",
    "- No Powershell: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v ${PWD}:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "- No Prompt de comando: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v %cd%:/home/jovyan/work jupyter/pyspark-notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar a vida eu coloco esse comando em um arquivo `inicia.sh`. Engenheiros, façam do jeito que preferirem!\n",
    "\n",
    "Agora abra esse notebook lá no container utilizando o link **com o token** que é exibido ao executar o comando!\n",
    "\n",
    "**Importante:** Se você já estiver visualizando esse notebook via Jupyter, pode ser necessário encerrar o processo para que o comando acima funcione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora configurar o insperautograder para que funcione dentro do container do Docker. Primeiro, instale e importe a biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/macielcalebe/insperautograding.git\n",
      "  Cloning https://github.com/macielcalebe/insperautograding.git to c:\\users\\pache\\appdata\\local\\temp\\pip-req-build-2amj91ot\n",
      "  Resolved https://github.com/macielcalebe/insperautograding.git to commit acdda51152774d9e1a979b426e41daa7a8a7793c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting python-dotenv (from insperautograder==0.2.0)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from insperautograder==0.2.0) (2.30.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from insperautograder==0.2.0) (8.5.0)\n",
      "Collecting ipywidgets (from insperautograder==0.2.0)\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.4/139.4 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: backcall in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (3.0.31)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (2.13.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.5.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython->insperautograder==0.2.0) (0.4.5)\n",
      "Collecting comm>=0.1.3 (from ipywidgets->insperautograder==0.2.0)\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets->insperautograder==0.2.0)\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets->insperautograder==0.2.0)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "     -------------------------------------- 215.0/215.0 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->insperautograder==0.2.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->insperautograder==0.2.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->insperautograder==0.2.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->insperautograder==0.2.0) (2022.12.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jedi>=0.16->ipython->insperautograder==0.2.0) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython->insperautograder==0.2.0) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython->insperautograder==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: asttokens in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython->insperautograder==0.2.0) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython->insperautograder==0.2.0) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from asttokens->stack-data->ipython->insperautograder==0.2.0) (1.16.0)\n",
      "Building wheels for collected packages: insperautograder\n",
      "  Building wheel for insperautograder (pyproject.toml): started\n",
      "  Building wheel for insperautograder (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for insperautograder: filename=insperautograder-0.2.0-py3-none-any.whl size=4467 sha256=0a82890befcfa4e222cc239a7c70e39a79052a228de380c54d348ff3908395ce\n",
      "  Stored in directory: C:\\Users\\pache\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ftkl47bb\\wheels\\d0\\99\\1c\\9d39e4f753118014618a414a556a782bf3facf6396735430c6\n",
      "Successfully built insperautograder\n",
      "Installing collected packages: widgetsnbextension, python-dotenv, jupyterlab-widgets, comm, ipywidgets, insperautograder\n",
      "Successfully installed comm-0.2.2 insperautograder-0.2.0 ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 python-dotenv-1.0.1 widgetsnbextension-4.0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/macielcalebe/insperautograding.git 'C:\\Users\\pache\\AppData\\Local\\Temp\\pip-req-build-2amj91ot'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\pache\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/macielcalebe/insperautograding.git\n",
    "\n",
    "import insperautograder.jupyter as ia\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie o arquivo .env para dentro da pasta que está vinculada ao container e veja se está funcionando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade            | De                        | Até                       |\n",
       "|---:|:---------------------|:--------------------------|:--------------------------|\n",
       "|  0 | newborn              | 2024-02-01 03:00:00+00:00 | 2024-05-30 03:00:00+00:00 |\n",
       "|  1 | select01             | 2024-02-08 03:00:00+00:00 | 2024-02-19 02:59:59+00:00 |\n",
       "|  2 | ddl                  | 2024-02-22 03:00:00+00:00 | 2024-02-27 02:59:59+00:00 |\n",
       "|  3 | dml                  | 2024-02-26 03:00:00+00:00 | 2024-03-03 02:59:59+00:00 |\n",
       "|  4 | group_having         | 2024-02-29 03:00:00+00:00 | 2024-03-12 02:59:59+00:00 |\n",
       "|  5 | views                | 2024-02-29 03:00:00+00:00 | 2024-03-20 02:59:59+00:00 |\n",
       "|  6 | agg_join             | 2024-02-29 03:00:00+00:00 | 2024-03-05 02:59:59+00:00 |\n",
       "|  7 | sql_review1          | 2024-03-11 03:00:00+00:00 | 2024-03-20 02:59:59+00:00 |\n",
       "|  8 | permissions          | 2024-03-18 03:00:00+00:00 | 2024-03-26 02:59:59+00:00 |\n",
       "|  9 | desafio_normalizacao | 2024-03-21 03:00:00+00:00 | 2024-04-15 02:59:59+00:00 |\n",
       "| 10 | ai_md_23_1           | 2024-03-25 03:00:00+00:00 | 2024-04-01 15:00:00+00:00 |\n",
       "| 11 | ai_md_23_2           | 2024-03-25 03:00:00+00:00 | 2024-04-01 15:00:00+00:00 |\n",
       "| 12 | ai_md_24_1           | 2024-04-01 03:00:00+00:00 | 2024-04-01 18:35:00+00:00 |\n",
       "| 13 | triggers             | 2024-04-18 03:00:00+00:00 | 2024-04-27 02:59:59+00:00 |\n",
       "| 14 | functional           | 2024-04-25 03:00:00+00:00 | 2024-05-04 02:59:59+00:00 |\n",
       "| 15 | spark                | 2024-05-02 03:00:00+00:00 | 2024-05-11 02:59:59+00:00 |\n",
       "| 16 | exercicios_spark     | 2024-05-06 03:00:00+00:00 | 2024-05-13 02:59:59+00:00 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "ia.tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Tarefa               |   Nota |\n",
       "|---:|:---------------------|-------:|\n",
       "|  0 | agg_join             |  10    |\n",
       "|  1 | ai_md_23_1           |   5    |\n",
       "|  2 | ai_md_23_2           |   6.25 |\n",
       "|  3 | ai_md_24_1           |   7.5  |\n",
       "|  4 | ddl                  |  10    |\n",
       "|  5 | desafio_normalizacao |  10    |\n",
       "|  6 | dml                  |  10    |\n",
       "|  7 | exercicios_spark     |   0    |\n",
       "|  8 | functional           |  10    |\n",
       "|  9 | group_having         |  10    |\n",
       "| 10 | newborn              |  10    |\n",
       "| 11 | permissions          |   5.38 |\n",
       "| 12 | select01             |  10    |\n",
       "| 13 | spark                |   7.14 |\n",
       "| 14 | sql_review1          |   3    |\n",
       "| 15 | triggers             |   2.5  |\n",
       "| 16 | views                |   5    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.grades(by=\"TASK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade   | Exercício   |   Peso |   Nota |\n",
       "|---:|:------------|:------------|-------:|-------:|\n",
       "|  0 | spark       | desafio1    |      1 |      0 |\n",
       "|  1 | spark       | desafio2    |      1 |      0 |\n",
       "|  2 | spark       | ex01        |      1 |     10 |\n",
       "|  3 | spark       | ex02        |      1 |     10 |\n",
       "|  4 | spark       | ex03        |      1 |     10 |\n",
       "|  5 | spark       | ex04        |      1 |     10 |\n",
       "|  6 | spark       | ex05        |      1 |     10 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.grades(task=\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opção 2 - Instalar `pyspark` diretamente\n",
    "\n",
    "Caso já tenha as dependências em sua máquina, faça:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "     -------------------------------------- 317.0/317.0 MB 2.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ------------------------------------- 200.5/200.5 kB 11.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488532 sha256=5c84a85c21d11ca481eea52099dfec447dc9bdf0adfa87e6907b93f0b1920f20\n",
      "  Stored in directory: c:\\users\\pache\\appdata\\local\\pip\\cache\\wheels\\80\\1d\\60\\2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\pache\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488532 sha256=1a2799290e50b37d065080cfcb1f7e22cf6092d83072dc6b67115211848271ce\n",
      "  Stored in directory: c:\\users\\pache\\appdata\\local\\pip\\cache\\wheels\\80\\1d\\60\\2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\pache\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\pache\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando o Spark\n",
    "\n",
    "Vamos iniciar o ambiente Spark. Para isso vamos:\n",
    "\n",
    "1) Criar um objeto de configuração do ambiente Spark. Nossa configuração será simples: vamos especificar que o nome da nossa aplicação Spark é \"Minha aplicação\", e que o *master node* é a máquina local, usando todos os *cores* disponíveis. Aplicações reais de Spark são configuradas de modo ligeiramente diferente: ao especificar o *master node* passamos uma URL real, com o endereço do nó gerente do *cluster* Spark.\n",
    "\n",
    "2) Vamos criar um objeto do tipo `SparkContext` com essa configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinhaAplicacao\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `SparkContext` é a nossa porta de entrada para o cluster Spark, ele será a raiz de todas as nossas operações com o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ChiPC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MinhaAplicacao</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MinhaAplicacao>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O link acima para a Spark UI provavelmente não funcionará porque ele se refere à porta 4040 interna do container (portanto a URL está com endereço interno). Porém fizemos o mapeamento da porta 4040 interna para a porta 4040 externa, logo você pode acessar o *dashboard* do Spark para monitorar seus *jobs* no endereço http://localhost:4040\n",
    "\n",
    "<center><img src=\"./img/spark_dashboard.png\" width=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, assim você vai conseguir testar seus programas com facilidade!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com RDDs\n",
    "\n",
    "Esse é o jeito \"Apache raiz\": Resilient Distributed Datasets (RDDs). Este é o principal objeto de processamento do Spark.\n",
    "\n",
    "Um RDD é criado à partir do objeto `SparkContext`. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos recuperar dois elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um RDD também pode ser criado a partir de um dataset (claro!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/memorias.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o rdd pode ser particionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de partições:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de partições: \", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos imaginar um RDD como uma coleção de itens, similar a uma grande lista. O que vem em cada item depende do arquivo original de dados. Para arquivos de texto, cada linha é um item.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html e sobre os RDDs em https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Açoes e Transformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto `rdd` é repleto de métodos para definir pipelines computacionais. Estes métodos se dividem em *actions* e *transformations*.\n",
    "\n",
    "*Transformations* são métodos que atuam no RDD e devolvem um \"novo\" RDD que está conectado ao RDD antigo. Por exemplo, vamos usar a *transformation* `map` para inverter a sequência de letras em cada linha: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverte_linha(linha):\n",
    "    return linha[::-1]\n",
    "\n",
    "rdd2 = rdd.map(inverte_linha)\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você consultar a Web UI do Spark (http://localhost:4040) vai ver que nada ainda aconteceu. Isto é assim porque o Spark é *lazy*: a computação só acontece quando um resultado de uma sequência de *transformations* é demandado por uma *action*.\n",
    "\n",
    "As *actions* são métodos que retornam dados para o seu programa. Por exemplo, a *action* `count` retorna o número de itens no RDD, e a *action* `take` permite coletar alguns itens no inicio do RDD, bem útil para debugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8843"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confira a Web UI e veja que agora temos um novo job completo!\n",
    "\n",
    "Vamos espiar as primeiras 20 linhas do documento original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's Memorias Postumas de Braz Cubas, by Machado de Assis\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and most',\n",
       " 'other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  You may copy it, give it away or re-use it under the terms of',\n",
       " 'the Project Gutenberg License included with this eBook or online at',\n",
       " \"www.gutenberg.org.  If you are not located in the United States, you'll have\",\n",
       " 'to check the laws of the country where you are located before using this ebook.',\n",
       " '',\n",
       " 'Title: Memorias Postumas de Braz Cubas',\n",
       " '',\n",
       " 'Author: Machado de Assis',\n",
       " '',\n",
       " 'Release Date: June 2, 2017 [EBook #54829]',\n",
       " '',\n",
       " 'Language: Portuguese',\n",
       " '',\n",
       " '',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK MEMORIAS POSTUMAS DE BRAZ CUBAS ***',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora as 20 primeiras linhas após a inversão de linha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"sissA ed odahcaM yb ,sabuC zarB ed samutsoP sairomeM s'grebnetuG tcejorP\",\n",
       " '',\n",
       " 'tsom dna setatS detinU eht ni erehwyna enoyna fo esu eht rof si kooBe sihT',\n",
       " 'snoitcirtser on tsomla htiw dna tsoc on ta dlrow eht fo strap rehto',\n",
       " 'fo smret eht rednu ti esu-er ro yawa ti evig ,ti ypoc yam uoY  .reveostahw',\n",
       " 'ta enilno ro kooBe siht htiw dedulcni esneciL grebnetuG tcejorP eht',\n",
       " \"evah ll'uoy ,setatS detinU eht ni detacol ton era uoy fI  .gro.grebnetug.www\",\n",
       " '.koobe siht gnisu erofeb detacol era uoy erehw yrtnuoc eht fo swal eht kcehc ot',\n",
       " '',\n",
       " 'sabuC zarB ed samutsoP sairomeM :eltiT',\n",
       " '',\n",
       " 'sissA ed odahcaM :rohtuA',\n",
       " '',\n",
       " ']92845# kooBE[ 7102 ,2 enuJ :etaD esaeleR',\n",
       " '',\n",
       " 'eseugutroP :egaugnaL',\n",
       " '',\n",
       " '',\n",
       " '*** SABUC ZARB ED SAMUTSOP SAIROMEM KOOBE GREBNETUG TCEJORP SIHT FO TRATS ***',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos discutir algumas açoes e transformações uteis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map`\n",
    "\n",
    "A transformação `map` recebe uma função e aplica esta função a cada item do RDD. A função deve receber um item e retornar um unico item. Como exemplo temos o uso de `map` acima para inverter a ordem das letras de cada linha.\n",
    "\n",
    "Note que no `map`, o RDD resultante tem o mesmo numero de elementos do RDD original.\n",
    "\n",
    "**Exercicio 1**: Crie uma função chamada conta_letras que recebe um rdd e retorna o rdd transformado onde cada linha tenha a contagem de letras da linha recebida. Por exemplo, a linha \"abacaxi\" deve ser transformada em 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 0, 74, 67, 74, 67, 76, 79, 0, 38, 0, 24, 0, 41, 0, 20, 0, 0, 77, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resultado_esperado: [72, 0, 74, 67, 74, 67, 76, 79, 0, 38, 0, 24, 0, 41, 0, 20, 0, 0, 77, 0 ...\n",
    "\n",
    "# Crie uma função chamada conta_letras que recebe um rdd e retorna o rdd transformado onde cada linha tenha a contagem de letras da linha recebida. Por exemplo, a linha \"abacaxi\" deve ser transformada em 7.\n",
    "\n",
    "def conta_letras(rdd):\n",
    "    return rdd.map(lambda x: len(x))\n",
    "\n",
    "conta_letras(rdd).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736e57ce9216437fa4496859d4e632cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex01', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"conta_letras\", task=\"spark\", question=\"ex01\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 2**: Crie uma função chamada transforma_minusculas que recebe um rdd e retorna o rdd transformado onde cada linha tenha todas as letras em minúsculas. Por exemplo, a linha \"AbaCaxi HOJE\" deve ser transformada em \"abacaxi hoje\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"project gutenberg's memorias postumas de braz cubas, by machado de assis\",\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and most',\n",
       " 'other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  you may copy it, give it away or re-use it under the terms of',\n",
       " 'the project gutenberg license included with this ebook or online at',\n",
       " \"www.gutenberg.org.  if you are not located in the united states, you'll have\",\n",
       " 'to check the laws of the country where you are located before using this ebook.',\n",
       " '',\n",
       " 'title: memorias postumas de braz cubas',\n",
       " '',\n",
       " 'author: machado de assis',\n",
       " '',\n",
       " 'release date: june 2, 2017 [ebook #54829]',\n",
       " '',\n",
       " 'language: portuguese',\n",
       " '',\n",
       " '',\n",
       " '*** start of this project gutenberg ebook memorias postumas de braz cubas ***',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crie uma função chamada transforma_minusculas que recebe um rdd e retorna o rdd transformado onde cada linha tenha todas as letras em minúsculas. Por exemplo, a linha \"AbaCaxi HOJE\" deve ser transformada em \"abacaxi hoje\".\n",
    "\n",
    "def transforma_minusculas(rdd):\n",
    "    return rdd.map(lambda x: x.lower())\n",
    "    \n",
    "transforma_minusculas(rdd).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5769c645b69444a1828814d71809f284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex02', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"transforma_minusculas\", task=\"spark\", question=\"ex02\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `flatMap`\n",
    "\n",
    "Esta transformação é similar ao `map`, porém a função passada para o `flatMap` pode retornar zero ou mais itens. Estes itens então são concatenados e o RDD resultante acaba tendo um número de itens diferente do RDD original.\n",
    "\n",
    "Por exemplo: suponha que queremos gerar uma lista de palavras do documento. Podemos fazê-lo da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def separa_palavras(linha):\n",
    "    return linha.strip().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe o resultado da aplicação da função `separa_palavras` com `map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['project',\n",
       "  \"gutenberg's\",\n",
       "  'memorias',\n",
       "  'postumas',\n",
       "  'de',\n",
       "  'braz',\n",
       "  'cubas,',\n",
       "  'by',\n",
       "  'machado',\n",
       "  'de',\n",
       "  'assis'],\n",
       " [],\n",
       " ['this',\n",
       "  'ebook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'and',\n",
       "  'most'],\n",
       " ['other',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with',\n",
       "  'almost',\n",
       "  'no',\n",
       "  'restrictions'],\n",
       " ['whatsoever.',\n",
       "  'you',\n",
       "  'may',\n",
       "  'copy',\n",
       "  'it,',\n",
       "  'give',\n",
       "  'it',\n",
       "  'away',\n",
       "  'or',\n",
       "  're-use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of'],\n",
       " ['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'license',\n",
       "  'included',\n",
       "  'with',\n",
       "  'this',\n",
       "  'ebook',\n",
       "  'or',\n",
       "  'online',\n",
       "  'at'],\n",
       " ['www.gutenberg.org.',\n",
       "  'if',\n",
       "  'you',\n",
       "  'are',\n",
       "  'not',\n",
       "  'located',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states,',\n",
       "  \"you'll\",\n",
       "  'have'],\n",
       " ['to',\n",
       "  'check',\n",
       "  'the',\n",
       "  'laws',\n",
       "  'of',\n",
       "  'the',\n",
       "  'country',\n",
       "  'where',\n",
       "  'you',\n",
       "  'are',\n",
       "  'located',\n",
       "  'before',\n",
       "  'using',\n",
       "  'this',\n",
       "  'ebook.'],\n",
       " [],\n",
       " ['title:', 'memorias', 'postumas', 'de', 'braz', 'cubas'],\n",
       " [],\n",
       " ['author:', 'machado', 'de', 'assis'],\n",
       " [],\n",
       " ['release', 'date:', 'june', '2,', '2017', '[ebook', '#54829]'],\n",
       " [],\n",
       " ['language:', 'portuguese'],\n",
       " [],\n",
       " [],\n",
       " ['***',\n",
       "  'start',\n",
       "  'of',\n",
       "  'this',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'memorias',\n",
       "  'postumas',\n",
       "  'de',\n",
       "  'braz',\n",
       "  'cubas',\n",
       "  '***'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['produced',\n",
       "  'by',\n",
       "  'laura',\n",
       "  'natal',\n",
       "  'rodriguez',\n",
       "  '&',\n",
       "  'marc',\n",
       "  \"d'hooghe\",\n",
       "  'at',\n",
       "  'free'],\n",
       " ['literature',\n",
       "  '(online',\n",
       "  'soon',\n",
       "  'in',\n",
       "  'an',\n",
       "  'extended',\n",
       "  'version,',\n",
       "  'also',\n",
       "  'linking'],\n",
       " ['to', 'free', 'sources', 'for', 'education', 'worldwide', '...', \"mooc's,\"],\n",
       " ['educational', 'materials,...)'],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map = rdd.map(lambda x: x.lower()).map(separa_palavras)\n",
    "rdd_map.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o resultado com `flatMap`, que é o resultado desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'braz',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'assis',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap = rdd.map(lambda x: x.lower()).flatMap(separa_palavras)\n",
    "rdd_flatMap.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a contagem de palavras no `rdd_flatMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64713"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare com o numero de itens no RDD original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8843"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 3**: Crie uma função chamada gera_bigramas que recebe um rdd e retorna o rdd transformado onde cada linha contém uma **tupla** com um *bigrama*: sequências de duas palavras consecutivas.\n",
    "\n",
    "Gere cada bigrama como uma **tupla** (p[i], p[i+1]). \n",
    "\n",
    "Sua função deve utilizar o comando `flatMap`. Para esse exercício é permitido utilizar um comando for dentro do flatmap. Por que isso não prejudica a performance?\n",
    "\n",
    "Além disso, teremos um pequeno problema nos bigramas - que problema é esse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', \"gutenberg's\"),\n",
       " (\"gutenberg's\", 'memorias'),\n",
       " ('memorias', 'postumas'),\n",
       " ('postumas', 'de'),\n",
       " ('de', 'braz'),\n",
       " ('braz', 'cubas,'),\n",
       " ('cubas,', 'by'),\n",
       " ('by', 'machado'),\n",
       " ('machado', 'de'),\n",
       " ('de', 'assis'),\n",
       " ('this', 'ebook'),\n",
       " ('ebook', 'is'),\n",
       " ('is', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'use'),\n",
       " ('use', 'of'),\n",
       " ('of', 'anyone'),\n",
       " ('anyone', 'anywhere'),\n",
       " ('anywhere', 'in'),\n",
       " ('in', 'the')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gera_bigramas(rdd):\n",
    "    return rdd.map(lambda x: x.lower()).map(lambda x: x.split()).flatMap(lambda x: [(x[i], x[i+1]) for i in range(len(x)-1)])\n",
    "\n",
    "rdd_bigramas = gera_bigramas(rdd)\n",
    "rdd_bigramas.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d4de926705427a8820a7852f4cacd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex03', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"gera_bigramas\", task=\"spark\", question=\"ex03\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema é que bigramas que cruzam linhas não serão contabilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `filter`\n",
    "\n",
    "A transformação `filter` recebe uma função que recebe um item e retorna True se o item deve ser mantido, e False se deve ser ignorado. Por exemplo, suponha que temos uma lista de palavras proibidas e queremos manter apenas as palavras permitidas da nossa lista de palavras em `rdd_flatMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'braz',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'assis',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'braz',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'assis',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eh_palavra_permitida(palavra):\n",
    "    palavras_proibidas = set([\"Braz\", \"Assis\"])\n",
    "    return palavra not in palavras_proibidas\n",
    "\n",
    "rdd_limpo = rdd_flatMap.filter(eh_palavra_permitida)\n",
    "rdd_limpo.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reduce`\n",
    "\n",
    "A *action* `reduce` apresenta o mesmo comportamento da operação `reduce` da biblioteca `functools` do Python. Ela recebe uma função de dois argumentos que retorna apenas um valor. O comportamento de `reduce` é aplicar esta função aos vários elementos do RDD de modo sucessivo, visando \"reduzir\" o RDD inteiro a um único valor, que é então retornado para o programa principal.\n",
    "\n",
    "Por exemplo, suponha que desejamos calcular a soma dos comprimentos de palavras. Podemos fazer o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317610"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.map(lambda x: len(x)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que como `reduce` é uma *action*, temos um resultado concreto.\n",
    "\n",
    "**Exercício 4**: Crie uma função chamada maior_palindromo que recebe um rdd com o texto e retorna a maior palavra palíndroma do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..........'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maior_palindromo(rdd):\n",
    "    def eh_palindromo(word):\n",
    "        return word == word[::-1]\n",
    "\n",
    "    palindromes = rdd.flatMap(lambda x: x.split()).filter(eh_palindromo).map(lambda x: (x, len(x)))\n",
    "    result = palindromes.reduce(lambda x, y: x if x[1] > y[1] else y)\n",
    "    \n",
    "    return result[0]\n",
    "\n",
    "maior_palindromo(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b429eed70bac4323ae8abe8e1749f8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex04', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"maior_palindromo\", task=\"spark\", question=\"ex04\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pares chave-valor\n",
    "\n",
    "Sempre que o RDD consistir em itens que sejam tuplas de dois elementos, Spark vai considerar que temos um par chave-valor por item. Isso nos permite realizar agregações por chave, abrindo a possibilidade de coletar dados agregados mais interessantes. \n",
    "\n",
    "#### `reduceByKey`\n",
    "Para essa tarefa, a transformação mais importante é a `reduceByKey`. Esta operação realiza uma agregação por chave (gerando uma lista de valores para aquela chave), e realiza uma redução da lista dos valores associados à chave.\n",
    "\n",
    "Por exemplo, o *hello world* do Spark: o conta-palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2593),\n",
       " ('de', 2139),\n",
       " ('que', 2097),\n",
       " ('e', 1944),\n",
       " ('o', 1831),\n",
       " ('não', 1063),\n",
       " ('um', 987),\n",
       " ('do', 744),\n",
       " ('da', 656),\n",
       " ('uma', 652)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 5**: Crie uma função chamada conta_bigramas que recebe um rdd com as tuplas de bigramas e retorna pares chave e valor dos 10 bigramas mais populares.\n",
    "\n",
    "**Atenção:** Apesar da resposta aparecer como se fossem listas, os elementos apresentados são tuplas em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '.'), 159),\n",
       " (('de', 'um'), 148),\n",
       " (('que', 'a'), 133),\n",
       " (('que', 'o'), 125),\n",
       " (('que', 'não'), 117),\n",
       " (('que', 'me'), 116),\n",
       " (('o', 'que'), 111),\n",
       " (('e', 'a'), 107),\n",
       " (('que', 'eu'), 107),\n",
       " (('de', 'uma'), 100)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conta_bigramas(rdd_bigramas):\n",
    "    return rdd_bigramas.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).takeOrdered(10, lambda x: -x[1])\n",
    "\n",
    "conta_bigramas(gera_bigramas(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028312dd063a4ecaa91937c50169ce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex05', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"conta_bigramas\", task=\"spark\", question=\"ex05\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `join`\n",
    "\n",
    "A transformação `join` realiza o *inner join* de dois RDDs. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 'chuchu')),\n",
       " ('b', (20, 'tomate')),\n",
       " ('b', (30, 'chuchu')),\n",
       " ('b', (30, 'tomate')),\n",
       " ('a', (10, 'banana')),\n",
       " ('a', (10, 'abacate'))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a = sc.parallelize([(\"a\", 10), (\"b\", 20), (\"b\", 30)])\n",
    "rdd_b = sc.parallelize([(\"a\", \"banana\"), (\"a\", \"abacate\"), (\"b\", \"chuchu\"), (\"b\", \"tomate\")])\n",
    "rdd_c = rdd_a.join(rdd_b)\n",
    "\n",
    "rdd_c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio 1**: Crie uma função chamada desafio1 que recebe um rdd e o transforma de modo que para cada bigrama, temos a seguinte informação:\n",
    "\n",
    "(bigrama, contagem_bigrama, contagem_palavra_1, contagem_palavra_2)\n",
    "\n",
    "**Atenção:** Apesar da resposta aparecer como se fossem listas, os elementos apresentados são tuplas em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/memorias.txt MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def desafio1(rdd):\n",
    "    # Seu código AQUI!\n",
    "    return rdd # Pode alterar o retorno!\n",
    "\n",
    "desafio1(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251c91f06f4547168b22ecbc8d7aee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar desafio1', style=ButtonStyle()), Output()), _dom_classes=('wi…"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"desafio1\", task=\"spark\", question=\"desafio1\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio 2**: Faça uma função chamada desafio2 que recebe um RDD no formato retornado pela função desafio1 e constrói um RDD com a seguinte informação:\n",
    "\n",
    "`(bigrama, (contagem_bigrama) /((contagem_palavra_1 + K) * (contagem_palavra_2 + K)))`\n",
    "\n",
    "para K = 10\n",
    "\n",
    "Por fim, retorne os 15 bigramas com maior valor associado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desafio2(rdd_d1):\n",
    "    # Seu código AQUI!\n",
    "    return rdd_d1 # Pode alterar o retorno!\n",
    "\n",
    "desafio2(desafio1(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.sender(answer=\"desafio2\", task=\"spark\", question=\"desafio2\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabarito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div id=\"gab_ex3\">**Exercício 3**</div>**\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "```python\n",
    "def gera_bigramas(rdd):\n",
    "    return rdd.map(lambda x: x.lower()) \\\n",
    "        .map(lambda x: x.split()) \\\n",
    "        .flatMap(lambda x: [(x[i], x[i+1]) for i in range(len(x)-1)])\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<div id=\"gab_ex4\">**Exercício 5**</div>**\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "```python\n",
    "def conta_bigramas(rdd_bigramas):\n",
    "    rdd_conta_bigramas = rdd_bigramas.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    return rdd_conta_bigramas.takeOrdered(10, lambda x: -x[1])\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
